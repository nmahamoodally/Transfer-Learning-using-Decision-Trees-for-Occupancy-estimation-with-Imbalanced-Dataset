{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.threshold_guess import compute_thresholds, cut\n",
    "from model.gosdt import GOSDT\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy\n",
    "from sklearn.model_selection import GridSearchCV , StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H355\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv(\"../experiments/datasets/H355.csv\")\n",
    "df.loc[df[df.labels.isin([ 3.0,4.0])].index, 'labels']=2.0\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "X= X.drop(['time'], axis=1)\n",
    "h = X.columns[:-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['co2', 'power1', 'power3', 'power4', 'power2', 'Temperature_CO2',\n",
       "       'Humidity_CO2', 'Contact_porte', 'Temperature_1', 'Humidity_1', 'COV',\n",
       "       'Temperature_12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co2</th>\n",
       "      <th>power1</th>\n",
       "      <th>power3</th>\n",
       "      <th>power4</th>\n",
       "      <th>power2</th>\n",
       "      <th>Temperature_CO2</th>\n",
       "      <th>Humidity_CO2</th>\n",
       "      <th>Contact_porte</th>\n",
       "      <th>Temperature_1</th>\n",
       "      <th>Humidity_1</th>\n",
       "      <th>COV</th>\n",
       "      <th>Temperature_12</th>\n",
       "      <th>Humidity_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>410.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.20</td>\n",
       "      <td>33.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>780.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>20.48</td>\n",
       "      <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1090.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.32</td>\n",
       "      <td>37.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>19.84</td>\n",
       "      <td>39.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>390.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.08</td>\n",
       "      <td>37.6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.84</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>670.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.56</td>\n",
       "      <td>36.8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>18.72</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1540.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.80</td>\n",
       "      <td>38.8</td>\n",
       "      <td>39.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>1480.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.48</td>\n",
       "      <td>45.2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>20.64</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2550.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.52</td>\n",
       "      <td>44.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19.36</td>\n",
       "      <td>42.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>390.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.40</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>780.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.68</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.48</td>\n",
       "      <td>35.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>801 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         co2  power1  power3  power4  power2  Temperature_CO2  Humidity_CO2  \\\n",
       "907    410.0    22.0    11.0     0.0     0.0             19.4          27.0   \n",
       "633    780.0    24.0   130.0     0.0     0.0             20.8          27.0   \n",
       "329   1090.0    82.0     0.0     0.0     0.0             20.2          32.0   \n",
       "965    390.0    22.0    11.0     0.0     0.0             18.8          30.0   \n",
       "153    670.0    22.0    11.0     0.0     0.0             18.6          31.0   \n",
       "...      ...     ...     ...     ...     ...              ...           ...   \n",
       "348   1540.0    22.0    24.0     0.0     0.0             20.8          32.0   \n",
       "1044  1480.0    73.0    94.0     0.0     0.0             20.6          37.0   \n",
       "90    2550.0    22.0    11.0     0.0    52.0             19.8          37.0   \n",
       "587    390.0    22.0    11.0     0.0     0.0             19.6          28.0   \n",
       "665    780.0    22.0   106.0     0.0     0.0             20.4          29.0   \n",
       "\n",
       "      Contact_porte  Temperature_1  Humidity_1   COV  Temperature_12  \\\n",
       "907             0.0          19.20        33.2  17.0           20.00   \n",
       "633             0.0          20.32        32.0  52.0           20.48   \n",
       "329             1.0          20.32        37.6  36.0           19.84   \n",
       "965             0.0          18.08        37.6  19.0           19.84   \n",
       "153             0.0          18.56        36.8  33.0           18.72   \n",
       "...             ...            ...         ...   ...             ...   \n",
       "348             0.0          20.80        38.8  39.0           20.00   \n",
       "1044            1.0          20.48        45.2  48.0           20.64   \n",
       "90              0.0          19.52        44.8  60.0           19.36   \n",
       "587             0.0          20.16        32.0  22.0           18.40   \n",
       "665             0.0          19.68        36.0  41.0           20.48   \n",
       "\n",
       "      Humidity_12  \n",
       "907          34.8  \n",
       "633          30.4  \n",
       "329          39.2  \n",
       "965          38.0  \n",
       "153          35.2  \n",
       "...           ...  \n",
       "348          42.0  \n",
       "1044         42.0  \n",
       "90           42.8  \n",
       "587          37.2  \n",
       "665          35.6  \n",
       "\n",
       "[801 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GOSDT model\n",
    "config = {\n",
    "            \"regularization\": 0.01, # regularization penalizes the tree with more leaves. We recommend to set it to relative high value to find a sparse tree.  \n",
    "            \"depth_budget\": 6,\n",
    "            \"time_limit\": 120, # training time limit in seconds\n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "\n",
    "acc = model.score(X_train, y_train)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "n_leaves = model.leaves()\n",
    "\n",
    "print(\"Training accuracy: {}\".format(acc))\n",
    "print(\"Testing accuracy: {}\".format(test_acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tree_structure = model.tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_leaf_nodes = model.tree.leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(test_accuracy)\n",
    "prediction = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we had to constrain our search to very simple trees. We're likely to get a more useful and accurate model when we allow our model to find more complicated trees (we do this with less constraint on depth and with smaller regularization). However, for real-valued datasets like COMPAS, GOSDT might not finish running within a couple minutes. An example is shown below where the depth budget is set to 6 and the regularization is set to 0.001 for the COMPAS dataset. After 60 seconds, GOSDT hits the time limit and returns the current best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GOSDT configuration\n",
    "# set a smaller regularization and a higher depth budget. All other hyparameters are same as before. \n",
    "config = {\n",
    "            \"regularization\": 0.001, \n",
    "            \"depth_budget\": 6,\n",
    "            \"time_limit\": 60, \n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "\n",
    "acc = model.score(X, y)\n",
    "n_leaves = model.leaves()\n",
    "\n",
    "print(\"Training accuracy: {}\".format(acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv(\"../experiments/datasets/H358_edited.csv\")\n",
    "df.loc[df[df.labels.isin([ 3.0,4.0])].index, 'labels']=2.0\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "X= X.drop(['time'], axis=1)\n",
    "h = X.columns[:-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "# train GOSDT model\n",
    "config = {\n",
    "            \"regularization\": 0.01, # regularization penalizes the tree with more leaves. We recommend to set it to relative high value to find a sparse tree.  \n",
    "            \"depth_budget\": 6,\n",
    "            \"time_limit\": 120, # training time limit in seconds\n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "\n",
    "acc = model.score(X_test, y_test)\n",
    "n_leaves = model.leaves()\n",
    "\n",
    "print(\"Test accuracy: {}\".format(acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(test_accuracy)\n",
    "prediction = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the time-out, GOSDT didn't find a very accurate tree because it couldn't finish optimizing. If it did finish optimizing, we might need to wait for a long time or use a large amount of memory. An alternative, presented in [our AAAI '22 paper](https://arxiv.org/abs/2112.00798), is to use threshold and lowerbound guesses to help GOSDT find a near optimal tree for COMPAS within 1 minute. The guessing process works as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Threshold guess: we first use a black box model, gradient boosted trees, to guess useful thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT parameters for threshold and lower bound guesses\n",
    "n_est = 40\n",
    "max_depth = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest slowdowns for decision trees is deciding how to binarize the continuous features. We had a separate column in our dataset for age <=20.5, age <=21.5, age <= 22.5, ... and so on. Using threshold guessing lets us prune the thresholds we use, by looking at which thresholds are used by gradient boosted trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess thresholds\n",
    "X_train = pd.DataFrame(X_train, columns=h)\n",
    "X_train_guessed, thresholds, header, threshold_guess_time = compute_thresholds(X_train.copy(), y_train, n_est, max_depth)\n",
    "X_train_guessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lower bound guess: we use the gradient boosted tree to get labels for the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine what labels the gradient boosted tree model would predict for each of the training points. The idea is that many good decision trees are likely to make similar predictions to the gradient boosted tree on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess lower bound\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "clf = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train_guessed, y_train.values.flatten())\n",
    "warm_labels = clf.predict(X_train_guessed)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "lb_time = elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the labels as a tmp file and return the path to it.\n",
    "labelsdir = pathlib.Path('/tmp/warm_lb_labels')\n",
    "labelsdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "labelpath = labelsdir / 'warm_label.tmp'\n",
    "labelpath = str(labelpath)\n",
    "pd.DataFrame(warm_labels).to_csv(labelpath, header=\"class_labels\",index=None) # TODO: verify this formats correctly for gosdt (shouldn't require headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. train GOSDT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the information from our guesses to speed up training GOSDT, while retaining some nice properties on the tree it returns (see our AAAI '22 paper for more information). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GOSDT model\n",
    "config = {\n",
    "            \"regularization\": 0.001, \n",
    "            \"depth_budget\": 6,\n",
    "            \"time_limit\": 60, \n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X_train_guessed, pd.DataFrame(y_train))\n",
    "\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results\n",
    "acc = model.score(X_train_guessed, y_train)\n",
    "n_leaves = model.leaves()\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(\"Training accuracy: {}\".format(acc))\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the threshold and lower bound guesses, GOSDT can find a near optimal tree within seconds. This tree is much more accurate than the tree we found earlier, when using much more restrictive parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 8)\n",
      "(214, 8)\n",
      "train set column names == test set column names: True\n",
      "gosdt reported successful execution\n",
      "training completed. 0.000/0.000/0.008 (user, system, wall), mem=0 MB\n",
      "bounds: [0.155738..0.155738] (0.000000) loss=0.148712, iterations=330\n",
      "evaluate the model, extracting tree and scores\n",
      "Model training time: 0.0\n",
      "Training accuracy: 0.8512880562060889\n",
      "Test accuracy: 0.883177570093458\n",
      "# of leaves: 6\n",
      "if co2<=675.0 = 1 and power<=28.0 = 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.016\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if co2<=675.0 = 1 and power<=28.0 != 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.001\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if Temperature<=19.800000190734863 = 1 and co2<=675.0 != 1 and power<=28.0 = 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.005\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if Temperature<=19.800000190734863 = 1 and co2<=675.0 != 1 and power<=28.0 != 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.005\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if Temperature<=19.800000190734863 != 1 and co2<=675.0 != 1 and power<=10.625 = 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.008\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if Temperature<=19.800000190734863 != 1 and co2<=675.0 != 1 and power<=10.625 != 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.114\n",
      "    complexity penalty: 0.001\n",
      "Confusion Matrix:\n",
      "  [[154   5   0]\n",
      " [  2  35   0]\n",
      " [  0  18   0]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../experiments/datasets/H355_edited.csv\")\n",
    "df.loc[df[df.labels.isin([ 3.0,4.0])].index, 'labels']=2.0\n",
    "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "X= X.drop(['time'], axis=1)\n",
    "h = X.columns[:-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021)\n",
    "n_est = 40\n",
    "max_depth = 1\n",
    "X_train = pd.DataFrame(X_train, columns=h)\n",
    "X_test = pd.DataFrame(X_test, columns=h)\n",
    "X_train_guessed, thresholds, header, threshold_guess_time = compute_thresholds(X_train.copy(), y_train, n_est, max_depth)\n",
    "X_test_guessed = cut(X_test.copy(), thresholds)\n",
    "X_test_guessed = X_test_guessed[header]\n",
    "print(X_train_guessed.shape)\n",
    "print(X_test_guessed.shape)\n",
    "print(\"train set column names == test set column names: {}\".format(list(X_train_guessed.columns)==list(X_test_guessed.columns)))\n",
    "\n",
    "# guess lower bound\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "clf = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train_guessed, y_train.values.flatten())\n",
    "warm_labels = clf.predict(X_train_guessed)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "lb_time = elapsed_time\n",
    "labelsdir = pathlib.Path('/tmp/warm_lb_labels')\n",
    "labelsdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "labelpath = labelsdir / 'warm_label.tmp'\n",
    "labelpath = str(labelpath)\n",
    "pd.DataFrame(warm_labels).to_csv(labelpath, header=\"class_labels\",index=None)\n",
    "config = {\n",
    "            \"regularization\": 0.001,\n",
    "            \"depth_budget\": 5,\n",
    "            \"time_limit\": 60,\n",
    "            \"warm_LB\": True,\n",
    "            \"path_to_labels\": labelpath,\n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X_train_guessed, pd.DataFrame(y_train))\n",
    "\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "train_acc = model.score(X_train_guessed, y_train)\n",
    "test_acc = model.score(X_test_guessed, y_test)\n",
    "n_leaves = model.leaves()\n",
    "n_nodes = model.nodes()\n",
    "time = model.utime\n",
    "\n",
    "print(\"Model training time: {}\".format(time))\n",
    "print(\"Training accuracy: {}\".format(train_acc))\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)\n",
    "\n",
    "prediction = model.predict(X_test_guessed)\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transfer Learning\n",
    "Integrating GOSDT with Simple TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../experiments/datasets/H355_edited.csv\")\n",
    "df.loc[df[df.labels.isin([2.0, 3.0,4.0])].index, 'labels']=1.0\n",
    "Xs, ys = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "Xs= Xs.drop(['time'], axis=1)\n",
    "h = Xs.columns[:-1]\n",
    "Xs.shape\n",
    "\n",
    "\n",
    "# read the dataset\n",
    "df = pd.read_csv(\"../experiments/datasets/H358_edited.csv\")\n",
    "df.loc[df[df.labels.isin([ 2.0,3.0,4.0])].index, 'labels']=1.0\n",
    "Xt, yt = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "Xt= Xt.drop(['time'], axis=1)\n",
    "h = Xt.columns\n",
    "\n",
    "# If train-test split is desired\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(Xs, ys, test_size=0.2, random_state=2021)\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(Xt, yt, test_size=0.8, random_state=2021)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train_t.reset_index(drop=True)\n",
    "X_train_s = X_train_s.reset_index(drop=True)\n",
    "y_train_t = y_train_t.reset_index(drop=True)\n",
    "y_train_s = y_train_s.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_s, X_train_t])\n",
    "y_train = pd.concat([y_train_s, y_train_t])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS-PC\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set column names == test set column names: True\n",
      "gosdt reported successful execution\n",
      "training completed. 0.000/0.000/0.026 (user, system, wall), mem=0 MB\n",
      "bounds: [0.056206..0.056206] (0.000000) loss=0.051522, iterations=997\n",
      "evaluate the model, extracting tree and scores\n",
      "Model training time: 0.0\n",
      "Training accuracy: 0.9484777517564403\n",
      "Test accuracy: 0.9579439252336449\n",
      "# of leaves: 4\n",
      "if Door_contact<=0.5 = 1 and power<=10.625 = 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.015\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if Door_contact<=0.5 != 1 and power<=10.625 = 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.004\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if co2<=505.0 = 1 and power<=10.625 != 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.001\n",
      "    complexity penalty: 0.001\n",
      "\n",
      "else if co2<=505.0 != 1 and power<=10.625 != 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.032\n",
      "    complexity penalty: 0.001\n",
      "Confusion Matrix:\n",
      "  [[150   9]\n",
      " [  0  55]]\n"
     ]
    }
   ],
   "source": [
    "n_est = 40\n",
    "max_depth = 1\n",
    "X_train = pd.DataFrame(X_train, columns=h)\n",
    "X_test = pd.DataFrame(X_test_s, columns=h)\n",
    "X_train_guessed, thresholds, header, threshold_guess_time = compute_thresholds(X_train.copy(), y_train, n_est, max_depth)\n",
    "X_test_guessed = cut(X_test_s.copy(), thresholds)\n",
    "X_test_guessed = X_test_guessed[header]\n",
    "\n",
    "print(\"train set column names == test set column names: {}\".format(list(X_train_guessed.columns)==list(X_test_guessed.columns)))\n",
    "\n",
    "X_train_s = X_train_s.reset_index(drop=True)\n",
    "X_train_s = X_train_guessed.iloc[:854]\n",
    "y_train_s = y_train.iloc[:854]\n",
    "X_train_t = X_train_t.reset_index(drop=True)\n",
    "X_train_t = X_train_guessed.iloc[-288:]\n",
    "y_train_t = y_train.iloc[-288:]\n",
    "X_train_s.shape\n",
    "\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    "clf = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train_s, y_train_s.values.flatten())\n",
    "warm_labels = clf.predict(X_train_s)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "lb_time = elapsed_time\n",
    "labelsdir = pathlib.Path('/tmp/warm_lb_labels')\n",
    "labelsdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "labelpath = labelsdir / 'warm_label.tmp'\n",
    "labelpath = str(labelpath)\n",
    "pd.DataFrame(warm_labels).to_csv(labelpath, header=\"class_labels\",index=None)\n",
    "\n",
    "\n",
    "config = {\n",
    "            \"regularization\": 0.001,\n",
    "            \"depth_budget\": 10,\n",
    "            \"time_limit\": 60,\n",
    "            \"warm_LB\": True,\n",
    "            \"path_to_labels\": labelpath,\n",
    "            \"similar_support\": False\n",
    "        }\n",
    "\n",
    "model = GOSDT(config)\n",
    "\n",
    "model.fit(X_train_s, pd.DataFrame(y_train_s))\n",
    "\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "train_acc = model.score(X_train_s, y_train_s)\n",
    "test_acc = model.score(X_test_guessed, y_test_s)\n",
    "n_leaves = model.leaves()\n",
    "n_nodes = model.nodes()\n",
    "time = model.utime\n",
    "\n",
    "print(\"Model training time: {}\".format(time))\n",
    "print(\"Training accuracy: {}\".format(train_acc))\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)\n",
    "\n",
    "prediction = model.predict(X_test_guessed)\n",
    "cm = confusion_matrix(y_test_s, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_guessed_t = cut(X_test_t.copy(), thresholds)\n",
    "X_test_guessed_t = X_test_guessed_t[header]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_guessed_t = X_test_guessed_t.reset_index(drop=True)\n",
    "y_test_t = y_test_t.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gosdt reported successful execution\n",
      "training completed. 0.000/0.000/0.004 (user, system, wall), mem=0 MB\n",
      "bounds: [0.208333..0.208333] (0.000000) loss=0.194444, iterations=321\n",
      "evaluate the model, extracting tree and scores\n",
      "Model training time: 0.0\n",
      "Training accuracy: 0.8055555555555556\n",
      "Test accuracy: 0.8038194444444444\n",
      "# of leaves: 4\n",
      "if Door_contact<=0.5 = 1 and co2<=475.0 = 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.094\n",
      "    complexity penalty: 0.003\n",
      "\n",
      "else if Door_contact<=0.5 != 1 and co2<=475.0 = 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.007\n",
      "    complexity penalty: 0.003\n",
      "\n",
      "else if co2<=475.0 != 1 and co2<=725.0 = 1 then:\n",
      "    predicted class: 1\n",
      "    misclassification penalty: 0.073\n",
      "    complexity penalty: 0.003\n",
      "\n",
      "else if co2<=475.0 != 1 and co2<=725.0 != 1 then:\n",
      "    predicted class: 0\n",
      "    misclassification penalty: 0.021\n",
      "    complexity penalty: 0.003\n",
      "Confusion Matrix:\n",
      "  [[727 113]\n",
      " [113 199]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "clf = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, random_state=42)\n",
    "clf.fit(X_train_t, y_train_t.values.flatten())\n",
    "warm_labels = clf.predict(X_train_t)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "lb_time = elapsed_time\n",
    "labelsdir = pathlib.Path('/tmp/warm_lb_labels')\n",
    "labelsdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "labelpath = labelsdir / 'warm_label.tmp'\n",
    "labelpath = str(labelpath)\n",
    "pd.DataFrame(warm_labels).to_csv(labelpath, header=\"class_labels\",index=None)\n",
    "\n",
    "model.fit(X_train_t, pd.DataFrame(y_train_t))\n",
    "\n",
    "print(\"evaluate the model, extracting tree and scores\", flush=True)\n",
    "train_acc = model.score(X_train_t, y_train_t)\n",
    "test_acc = model.score(X_test_guessed_t, y_test_t)\n",
    "n_leaves = model.leaves()\n",
    "n_nodes = model.nodes()\n",
    "time = model.utime\n",
    "\n",
    "print(\"Model training time: {}\".format(time))\n",
    "print(\"Training accuracy: {}\".format(train_acc))\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "print(\"# of leaves: {}\".format(n_leaves))\n",
    "print(model.tree)\n",
    "\n",
    "prediction = model.predict(X_test_guessed_t)\n",
    "cm = confusion_matrix(y_test_t, prediction)\n",
    "print(\"Confusion Matrix:\\n \", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we find a reasonably accurate model quite quickly on datasets as complicated as FICO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading our tutorial. Please do try out our methods with different parameters and datasets. Happy tree training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
